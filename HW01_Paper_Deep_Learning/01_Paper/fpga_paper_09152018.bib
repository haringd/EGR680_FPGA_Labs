@INPROCEEDINGS{7943929, 
author={P. R. Gankidi and J. Thangavelautham}, 
booktitle={2017 IEEE Aerospace Conference}, 
title={FPGA architecture for deep learning and its application to planetary robotics}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Autonomous control systems onboard planetary rovers and spacecraft benefit from having cognitive capabilities like learning so that they can adapt to unexpected situations in-situ. Q-learning is a form of reinforcement learning and it has been efficient in solving certain class of learning problems. However, embedded systems onboard planetary rovers and spacecraft rarely implement learning algorithms due to the constraints faced in the field, like processing power, chip size, convergence rate and costs due to the need for radiation hardening. These challenges present a compelling need for a portable, low-power, area efficient hardware accelerator to make learning algorithms practical onboard space hardware. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN). This method matches the massive parallelism inherent in neural network software with the fine-grain parallelism of an FPGA hardware thereby dramatically reducing processing time. Mars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices for image processing, pyrotechnic operation control and obstacle avoidance. We simulate and program our architecture on a Xilinx Virtex 7 FPGA. The architectural implementation for a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator has been demonstrated. The results show up to a 43-fold speed up by Virtex 7 FPGAs compared to a conventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed architecture using the Symphony simulator and compiler from Xilinx, and evaluate the performance and power consumption.}, 
keywords={aerospace computing;aerospace robotics;collision avoidance;field programmable gate arrays;learning (artificial intelligence);mobile robots;multilayer perceptrons;planetary rovers;space vehicles;FPGA architecture;deep learning;planetary robotics;autonomous control systems;planetary rovers;spacecraft;Q-learning;reinforcement learning;artificial neural networks;ANN;Xilinx-Space-grade Virtex FPGA devices;image processing;pyrotechnic operation control;obstacle avoidance;multilayer perception;MLP;Mathematical model;Field programmable gate arrays;Computer architecture;Robots;Hardware;Neural networks;Learning (artificial intelligence)}, 
doi={10.1109/AERO.2017.7943929}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7965030, 
author={M. Bacis and G. Natale and E. Del Sozzo and M. D. Santambrogio}, 
booktitle={2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
title={A pipelined and scalable dataflow implementation of convolutional neural networks on FPGA}, 
year={2017}, 
volume={}, 
number={}, 
pages={90-97}, 
abstract={Convolutional Neural Network (CNN) is a deep learning algorithm extended from Artificial Neural Network (ANN) and widely used for image classification and recognition, thanks to its invariance to distortions. The recent rapid growth of applications based on deep learning algorithms, especially in the context of Big Data analytics, has dramatically improved both industrial and academic research and exploration of optimized implementations of CNNs on accelerators such as GPUs, FPGAs and ASICs, as general purpose processors can hardly meet the ever increasing performance and energy-efficiency requirements. FPGAs in particular are one of the most attractive alternative, as they allow the exploitation of the implicit parallelism of the algorithm and the acceleration of the different layers of a CNN with custom optimizations, while retaining extreme flexibility thanks to their reconfigurability. In this work, we propose a methodology to implement CNNs on FPGAs in a modular, scalable way. This is done by exploiting the dataflow pattern of convolutions, using an approach derived from previous work on the acceleration of Iterative Stencil Loops (ISLs), a computational pattern that shares some characteristics with convolutions. Furthermore, this approach allows the implementation of a high-level pipeline between the different network layers, resulting in an increase of the overall performance when the CNN is employed to process batches of multiple images, as it would happen in real-life scenarios.}, 
keywords={data flow computing;feedforward neural nets;field programmable gate arrays;neural chips;pipeline processing;pipelined dataflow implementation;convolutional neural networks;FPGA;CNN;deep learning algorithm;artificial neural network;iterative stencil loops;ISL;field programmable gate arrays;Field programmable gate arrays;Acceleration;Feature extraction;Neural networks;Arrays;Neurons;Program processors;Field Programmable Gate Arrays;Convolu- tional Neural Networks;Dataflow Architectures}, 
doi={10.1109/IPDPSW.2017.44}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6339372, 
author={D. Sheffield and M. Anderson and K. Keutzer}, 
booktitle={22nd International Conference on Field Programmable Logic and Applications (FPL)}, 
title={Automatic generation of application-specific accelerators for FPGAs from python loop nests}, 
year={2012}, 
volume={}, 
number={}, 
pages={567-570}, 
abstract={We present Three Fingered Jack, a highly productive approach to mapping vectorizable applications to the FPGA. Our system applies traditional dependence analysis and reordering transformations to a restricted set of Python loop nests. It does this to uncover parallelism and divide computation between multiple parallel processing elements (PEs) that are automatically generated through high-level synthesis of the optimized loop body. Design space exploration on the FPGA proceeds by varying the number of PEs in the system. Over four benchmark kernels, our system achieves 3× to 6× relative to soft-core C performance.}, 
keywords={field programmable gate arrays;logic design;parallel processing;automatic generation;application-specific accelerators;FPGA;python loop nests;three-fingered Jack;multiple parallel processing elements;high-level synthesis;optimized loop body;design space exploration;benchmark kernels;soft-core C performance;Field programmable gate arrays;Kernel;Benchmark testing;Vectors;Random access memory;Image color analysis;Table lookup}, 
doi={10.1109/FPL.2012.6339372}, 
ISSN={1946-147X}, 
month={Aug},}
@misc{Learning,
	mendeley-groups = {FPGA{\_}EGR680{\_}F18},
	title = {{Learning | Definition of Learning by Merriam-Webster}},
	url = {https://www.merriam-webster.com/dictionary/learning},
	urldate = {2018-09-26}
}
